{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Utility-Functions\" data-toc-modified-id=\"Utility-Functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Utility Functions</a></div><div class=\"lev1 toc-item\"><a href=\"#MNIST\" data-toc-modified-id=\"MNIST-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>MNIST</a></div><div class=\"lev2 toc-item\"><a href=\"#Deep-Autoencoder\" data-toc-modified-id=\"Deep-Autoencoder-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Deep Autoencoder</a></div><div class=\"lev2 toc-item\"><a href=\"#Shallow-Autoencoder\" data-toc-modified-id=\"Shallow-Autoencoder-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Shallow Autoencoder</a></div><div class=\"lev1 toc-item\"><a href=\"#Denoising-Autoencoder\" data-toc-modified-id=\"Denoising-Autoencoder-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Denoising Autoencoder</a></div><div class=\"lev1 toc-item\"><a href=\"#Sparse-Autoencoders\" data-toc-modified-id=\"Sparse-Autoencoders-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Sparse Autoencoders</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.datasets import mnist\n",
    "from keras.regularizers import l1\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autoencoder_outputs(autoencoder, n, dims):\n",
    "    decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "    # Number of example digits to show\n",
    "    n = 5\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "    for i in range(n):\n",
    "        # plot original image\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(x_test[i].reshape(*dims))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        if i == n/2:\n",
    "            ax.set_title('Original Images')\n",
    "\n",
    "        # plot reconstruction \n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(decoded_imgs[i].reshape(*dims))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        if i == n/2:\n",
    "            ax.set_title('Reconstructed Images')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "We will be working with MNIST, a collection of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "To get a feel for the API of Keras, you will start by implementing a regular Neural Net, i.e. predict the label. If everything went correctly, the accuracy should be abover 0.9.\n",
    "\n",
    "Remember, a regular neural net has the following transformations:\n",
    "\n",
    "$$ \\mathbf{h} = \\sigma(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) $$\n",
    "$$ \\mathbf{y} = \\sigma(\\mathbf{W}_2\\mathbf{h} + \\mathbf{b}_2) $$\n",
    "\n",
    "Basic Keras documentation can be found here: https://keras.io/#getting-started-30-seconds-to-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_size = x_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "hidden_size = 32\n",
    "nr_epochs = 5\n",
    "activation_hidden = 'relu'\n",
    "activation_out = 'softmax'\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "##### DEFINE NEURAL NET\n",
    "model = Sequential()\n",
    "\n",
    "#########\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Autoencoder\n",
    "First we are going to look at a vanilla one-layer auto encoder. Here we simply go from the input to the bottleneck (hidden layer) back to the output. We denote the reconstructed output as $\\mathbf{\\hat{x}}$.\n",
    "\n",
    "In mathematical terms:\n",
    "\n",
    "$$ \\mathbf{h} = \\sigma(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) $$\n",
    "$$ \\mathbf{\\hat{x}} = \\sigma(\\mathbf{W}_2\\mathbf{h} + \\mathbf{b}_2) $$\n",
    "\n",
    "Where we try to minimize:\n",
    "\n",
    "$$ \\mathcal{L} = \\frac{1}{N} \\sum^N_{i=1}||\\mathbf{x}_i - \\mathbf{\\hat{x}}_i||^2 $$\n",
    "\n",
    "In different words, the average reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_size = x_train.shape[1]\n",
    "code_size = 32\n",
    "nr_epochs = 3\n",
    "activation_hidden = 'relu'\n",
    "activation_out = 'sigmoid'\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "##### DEFINE SHALLOW AUTOENCODER\n",
    "\n",
    "\n",
    "\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the reconstructed digits to see if our model has learned the correct transformations. If the output is almost the same as the input, the model performed well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autoencoder_outputs(model, 5, (28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the weights to see how the transformations approximately look like, if done correctly the weights correspond to some kind of pseudo-digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()[0].T\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i + 1)\n",
    "    plt.imshow(weights[i+0].reshape(28, 28))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Autoencoder\n",
    "Next up, a deep auto encoder, imagine here that a single network produces the bottleneck, while another network produces the output (in essence it is adding two lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_size = x_train.shape[1]\n",
    "hidden_size = 128\n",
    "code_size = 32\n",
    "nr_epochs = 3\n",
    "activation_hidden = 'relu'\n",
    "activation_out = 'sigmoid'\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "#### DEFINE DEEP AUTOENCODER\n",
    "\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autoencoder_outputs(model, 5, (28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()[0].T\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i + 1)\n",
    "    plt.imshow(weights[i+20].reshape(28, 28))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the weights for the deep autoencoder are less 'pronounced'. Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Denoising Autoencoder\n",
    "A very interesting application of auto encoders is that of denoising. Recover the original input given a source of noise over the input. First, we need to add noise to the input, we can use standard Gaussian noise for that (https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.normal.html). Do not forget to scale the noise when adding it to the input, otherwise results will be sub-optimal! \n",
    "\n",
    "In mathematical terms, our input becomes:\n",
    "\n",
    "$$ \\mathbf{\\tilde{x}} = \\mathbf{x} + \\lambda \\cdot \\boldsymbol{\\epsilon} \\quad \\text{with} \\, \\boldsymbol{\\epsilon}\\sim \\mathcal{N}(0,1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define noise scaling factor\n",
    "noise_factor = 0.4\n",
    "\n",
    "#### ADD NOISE TO THE INPUT\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "# Keep the pixels in a valid range\n",
    "x_train_noisy = np.clip(x_train_noisy, 0.0, 1.0)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0.0, 1.0)\n",
    "\n",
    "# Plot images\n",
    "n = 5\n",
    "plt.figure(figsize=(10, 4.5))\n",
    "for i in range(n):\n",
    "    # plot Original image\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == n/2:\n",
    "        ax.set_title('Original Images')\n",
    "\n",
    "    # plot Noisy image \n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == n/2:\n",
    "        ax.set_title('Noisy Input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went correctly, you should see the corrupted version of the digits below the actual ones. Now it is time to train the model to do the denoising, think carefully about the correct input-output structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_size = x_train.shape[1]\n",
    "hidden_size = 128\n",
    "code_size = 32\n",
    "nr_epochs = 10\n",
    "activation_hidden = 'relu'\n",
    "activation_out = 'sigmoid'\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "# DEFINE DENOISING AUTOENCODING HERE\n",
    "\n",
    "\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result of the denoising autoencoder\n",
    "n = 5\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "images = model.predict(x_test_noisy)\n",
    "\n",
    "for i in range(n):\n",
    "    # Plot original image\n",
    "    ax = plt.subplot(3, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == n/2:\n",
    "        ax.set_title('Original Images')\n",
    "\n",
    "    # Plot noisy image \n",
    "    ax = plt.subplot(3, n, i + 1 + n)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == n/2:\n",
    "        ax.set_title('Noisy Input')\n",
    "        \n",
    "    # Plot reconstructed image \n",
    "    ax = plt.subplot(3, n, i + 1 + 2*n)\n",
    "    plt.imshow(images[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == n/2:\n",
    "        ax.set_title('Autoencoder Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sparse Autoencoders\n",
    "Now we are going to make sparse autoencoders, remember that the only thing that changes is that we want to constrain our models to use less hidden units per example! \n",
    "\n",
    "Tip: check out https://keras.io/regularizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = x_train.shape[1]\n",
    "hidden_size = 128\n",
    "code_size = 32\n",
    "nr_epochs = 10\n",
    "activation_hidden = 'relu'\n",
    "activation_out = 'sigmoid'\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "#### DEFINE DENOISING AUTOENCODER\n",
    "\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_autoencoder_outputs(encoder_regularized, 5, (28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regular auto-encoder for 20 epochs for fair comparison\n",
    "input_img = Input(shape=(input_size,))\n",
    "code = Dense(code_size, activation='relu')(input_img)\n",
    "output_img = Dense(input_size, activation='sigmoid')(code)\n",
    "\n",
    "autoencoder_standard = Model(input_img, output_img)\n",
    "autoencoder_standard.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history_standard = autoencoder_standard.fit(x_train, x_train, epochs=5)\n",
    "\n",
    "encoder_standard = Model(input_img, code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare sparse and regular activations\n",
    "First, we compute the average activation of both models. Afterwards, we plot the distribution of the activations for both models. Note the amount of unused activations in the sparse autoencoder, exactly what we wanted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average activation of regular model:')\n",
    "print(encoder_standard.predict(x_test).mean())\n",
    "print('Average activation of sparse model:')\n",
    "print(encoder_regularized.predict(x_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scores = encoder_standard.predict(x_test).ravel()\n",
    "regularized_scores = encoder_regularized.predict(x_test).ravel()\n",
    "sns.distplot(standard_scores, hist=False, label='standard model')\n",
    "sns.distplot(regularized_scores, hist=False, label='regularized model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
