{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning training - Young Mavericks 17/05/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import gym\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant resources and documentation\n",
    "Most topics we discuss in this training come from the book by Sutton and Barto [1]. Both researchers have been involved in the field of RL for many years, and they compiled the book as a complete introduction. It has become a standard textbook in most RL courses from universities across the world.\n",
    "\n",
    "Additionally, we're using the following packages:\n",
    " - [OpenAI gym](https://github.com/openai/gym)\n",
    " - [numpy](https://docs.scipy.org/doc/numpy/reference/)\n",
    "\n",
    "\n",
    "[1] Sutton and Barto 2018 - Reinforcement Learning: An Introduction http://incompleteideas.net/book/the-book-2nd.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The OpenAI gym environment\n",
    "We'll use an example environment also found in Sutton & Barto's book: the grid world. Since `gym` is the de-facto standard package for RL environment, let's familiarize ourselves with its inner workings. Don't forget to check the documentation for relevant functions, attributes and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridworld import GridworldEnv\n",
    "\n",
    "# Don't change to WindyGridworldEnv unless you know what you're doing!\n",
    "from windy_gridworld import WindyGridworldEnv\n",
    "\n",
    "env = GridworldEnv() # Loads the grid world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your turn: reset the environment and draw it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your turn: find out the following information: how many actions are there? How many states are there?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your turn: let an agent take a sequence of 10 random actions in the world and print observations. What does the format represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your turn: Take some time to figure out what the environment's dynamics are and where they are stored. Figure out what the format represents and try to relate it to what was explained in the slides. Note that this is a deterministic environment. What would a stochastic environment look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "As a first step, we're going to implement _policy evaluation_. This algorithm requires a policy $\\pi(s)$, and in turn calculates the value function $V(s)$. Note that this is purely an evaluation: no improvements on the evluation are made (yet). As a start, we initialize $\\pi(s)$ randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Arguments:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: the OpenAI environment. \n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run your code, does it make sense?\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "V = policy_eval(random_policy, env)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gridworld_value(V):\n",
    "    plt.figure()\n",
    "    c = plt.pcolormesh(V, cmap='gray')\n",
    "    plt.colorbar(c)\n",
    "    plt.gca().invert_yaxis()  # In the array, first row = 0 is on top\n",
    "\n",
    "# Making a plot always helps\n",
    "plot_gridworld_value(V.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether your value function is in the correct shape!\n",
    "v = policy_eval(random_policy, env)\n",
    "assert v.shape == (env.nS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether the values are approximately correct!\n",
    "expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n",
    "assert np.allclose(v, expected_v, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy Iteration\n",
    "Now, use the policy evaluation in combination with policy improvement to implement policy iteration.\n",
    "\n",
    "Policy iteration will find a good policy for our environment, improving on the random policy we gave it in the beginning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "            \n",
    "        V = policy_eval(policy, env, discount_factor)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what it does\n",
    "policy, v = policy_improvement(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "def print_grid_policy(policy, symbols=[\"^\", \">\", \"v\", \"<\"]):\n",
    "    symbols = np.array(symbols)\n",
    "    for row in policy:\n",
    "        print(\"\".join(symbols[row]))\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n",
    "plot_gridworld_value(v.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the discount factor\n",
    "policy_discount, v_discount = policy_improvement(env, discount_factor=0.9)\n",
    "assert np.allclose(v_discount, np.array([\n",
    "    0.0, -1.0, -1.9, -2.71, \n",
    "    -1.0, -1.9, -2.71, -1.9, \n",
    "    -1.9, -2.71, -1.9, -1.0, \n",
    "    -2.71, -1.9, -1.0, 0.0\n",
    "]), atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "Next, implement the value iteration algorithm on the same environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    \n",
    "    ## YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh let's test again\n",
    "# Let's see what it does\n",
    "policy, v = value_iteration(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n",
    "orig_policy_eval = policy_eval\n",
    "del policy_eval\n",
    "try:\n",
    "    \n",
    "    policy, v = value_iteration(env)\n",
    "    assert np.allclose(v, np.array([0, -1, -2, -3, -1, -2, -3, -2, -2, -3, -2, -1, -3, -2, -1, 0]), atol=1e-3)\n",
    "    # With this one it should be easy to get discount right as they do not pass it\n",
    "    policy_discount, v_discount = value_iteration(env, discount_factor=0.9)\n",
    "    assert np.allclose(v_discount, np.array([\n",
    "        0.0, -1.0, -1.9, -2.71, \n",
    "        -1.0, -1.9, -2.71, -1.9, \n",
    "        -1.9, -2.71, -1.9, -1.0, \n",
    "        -2.71, -1.9, -1.0, 0.0\n",
    "    ]), atol=1e-3)\n",
    "\n",
    "finally:\n",
    "    policy_eval = orig_policy_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "Now, try to compare the two training methods with a random baseline. This means we can play around a bit with the settings of the environment as well, and see where the trade-off (if there is one) is between the two. \n",
    "\n",
    "Take the following approach:\n",
    "    1. Define a metric to evaluate how good a policy is on a level (i.e. how long it takes to complete a level, total reward)\n",
    "    2. Run the algorithm multiple times on the standard environment and measure the metric\n",
    "    3. Check the difference in performance.\n",
    "    \n",
    "Additionally, try to answer the following questions:\n",
    " - What happens if you terminate training of either policy or value iteration before convergence. How good are the trained policies? To they scale linearly in metric performance during training?\n",
    " - What happens if you increase the size of the environment? Can you make an estimation for how long it takes for algorithms to converge?\n",
    " - How would you speed up on the (naive) implementation?\n",
    " - Can you guess where neural networks are being used in state of the art models?\n",
    " - Can you adjust the GridWorldEnv environment itself in other ways? For instance, see what happens more terminal states are added, or removed?\n",
    " - What happens if we turn around the reward system: instead of giving -1 for every step you failed to reach a terminal state, you know achieve a +1 upon succesful completion.\n",
    " \n",
    " ### Try a different environment\n",
    " Thanks to the usage of `gym`, we can easily switch between different environments. In the code, there is also a WindyGridWorldEnv, is your code general enough to train on this level without much rewriting? How is this environment different? What differences do you see in training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_argmax(X):\n",
    "    \"\"\"\n",
    "    Perform an argmax, but break ties randomly. \n",
    "    \"\"\"\n",
    "    return np.random.choice(np.flatnonzero(X == X.max()))\n",
    "        \n",
    "def try_policy(N, policy):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        N: how many times to run the simulation\n",
    "        policy: policy to be tested\n",
    "    returns:\n",
    "        Average score of your metric\n",
    "    \"\"\"\n",
    "    \n",
    "    ## YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it\n",
    "N = 20\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "mean_random = try_policy(N, random_policy)\n",
    "mean_v_policy =  try_policy(N, policy) # policy is one you defined earlier!\n",
    "\n",
    "print(f\"Mean score of random policy: {mean_random}\")\n",
    "print(f\"Mean score of learned policy: {mean_v_policy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
